{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: MRJob lab\n",
    "type: lab\n",
    "duration: \"1:25\"\n",
    "creator:\n",
    "    name: Francesco Mosconi\n",
    "    city: SF\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) MRJob Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In the past lab we've used a Virtual Machine to run Map Reduce jobs on native hadoop. As you may have understood, it's quite cumbersome and complicated.\n",
    "\n",
    "Luckily we don't have to do that, because our friends at Yelp developed a great open source python library that wraps around hadoop streaming called [MRJob](https://github.com/Yelp/mrjob).\n",
    "\n",
    "This is already installed in your VM, but you can also install it locally if you prefer, using:\n",
    "\n",
    "`pip install mrjob`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mrjob\n",
      "  Downloading mrjob-0.5.2-py2.py3-none-any.whl (272kB)\n",
      "Collecting google-api-python-client>=1.5.0 (from mrjob)\n",
      "  Downloading google_api_python_client-1.5.1-py2.py3-none-any.whl (50kB)\n",
      "Collecting filechunkio (from mrjob)\n",
      "  Downloading filechunkio-1.6.tar.gz\n",
      "Requirement already satisfied (use --upgrade to upgrade): PyYAML>=3.08 in c:\\anaconda2\\lib\\site-packages (from mrjob)\n",
      "Requirement already satisfied (use --upgrade to upgrade): boto>=2.35.0 in c:\\anaconda2\\lib\\site-packages (from mrjob)\n",
      "Collecting uritemplate<1,>=0.6 (from google-api-python-client>=1.5.0->mrjob)\n",
      "  Downloading uritemplate-0.6.tar.gz\n",
      "Collecting oauth2client (from google-api-python-client>=1.5.0->mrjob)\n",
      "  Downloading oauth2client-2.2.0.tar.gz (70kB)\n",
      "Requirement already satisfied (use --upgrade to upgrade): six<2,>=1.6.1 in c:\\anaconda2\\lib\\site-packages (from google-api-python-client>=1.5.0->mrjob)\n",
      "Collecting httplib2<1,>=0.8 (from google-api-python-client>=1.5.0->mrjob)\n",
      "  Downloading httplib2-0.9.2.zip (210kB)\n",
      "Collecting simplejson>=2.5.0 (from uritemplate<1,>=0.6->google-api-python-client>=1.5.0->mrjob)\n",
      "  Downloading simplejson-3.8.2-cp27-cp27m-win_amd64.whl (66kB)\n",
      "Requirement already satisfied (use --upgrade to upgrade): pyasn1>=0.1.7 in c:\\anaconda2\\lib\\site-packages (from oauth2client->google-api-python-client>=1.5.0->mrjob)\n",
      "Collecting pyasn1-modules>=0.0.5 (from oauth2client->google-api-python-client>=1.5.0->mrjob)\n",
      "  Downloading pyasn1_modules-0.0.8-py2.py3-none-any.whl\n",
      "Collecting rsa>=3.1.4 (from oauth2client->google-api-python-client>=1.5.0->mrjob)\n",
      "  Downloading rsa-3.4.2-py2.py3-none-any.whl (46kB)\n",
      "Building wheels for collected packages: filechunkio, uritemplate, oauth2client, httplib2\n",
      "  Running setup.py bdist_wheel for filechunkio: started\n",
      "  Running setup.py bdist_wheel for filechunkio: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Owner\\AppData\\Local\\pip\\Cache\\wheels\\37\\fe\\61\\9fa0bcec2b5c84fe6b4c3ab9e1afc4b8aba74321185cf0d49d\n",
      "  Running setup.py bdist_wheel for uritemplate: started\n",
      "  Running setup.py bdist_wheel for uritemplate: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Owner\\AppData\\Local\\pip\\Cache\\wheels\\2d\\dc\\57\\124fcb62028d04cf74f6c7f5261f5deb29b3f6022eec179064\n",
      "  Running setup.py bdist_wheel for oauth2client: started\n",
      "  Running setup.py bdist_wheel for oauth2client: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Owner\\AppData\\Local\\pip\\Cache\\wheels\\af\\ae\\a0\\a103f5040adac297bae0c0b4a4e45f291be4c5ea8ca161fe42\n",
      "  Running setup.py bdist_wheel for httplib2: started\n",
      "  Running setup.py bdist_wheel for httplib2: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Owner\\AppData\\Local\\pip\\Cache\\wheels\\c7\\67\\60\\e0be8ccfc1e08f8ff1f50d99ea5378e204580ea77b0169fb55\n",
      "Successfully built filechunkio uritemplate oauth2client httplib2\n",
      "Installing collected packages: simplejson, uritemplate, httplib2, pyasn1-modules, rsa, oauth2client, google-api-python-client, filechunkio, mrjob\n",
      "Successfully installed filechunkio-1.6 google-api-python-client-1.5.1 httplib2-0.9.2 mrjob-0.5.2 oauth2client-2.2.0 pyasn1-modules-0.0.8 rsa-3.4.2 simplejson-3.8.2 uritemplate-0.6\n"
     ]
    }
   ],
   "source": [
    "!pip install mrjob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRJob\n",
    "\n",
    "While Hadoop streaming is a simple way to do simple map-reduce tasks, it's complicated to use and not really friendly when things fail and we have to debug our code. Also, if we wanted to do a join from two different sets of data, it would be complicate to handle both with a single mapper, while it'd be much easier to have two separate mappers and one reducer.\n",
    "\n",
    "_MRJob_ is a library written and maintained by YELP that allows us to write map reduce jobs in python.\n",
    "\n",
    "Here's the word count map reduce, rewritten using MRJob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%file wordfrequuency\n",
    "\n",
    "\"\"\"The classic MapReduce job: count the frequency of words.\n",
    "\"\"\"\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "\n",
    "class MRWordFreqCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        for word in WORD_RE.findall(line):\n",
    "            yield (word.lower(), 1)\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        yield (word, sum(counts))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "     MRWordFreqCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that it's almost trivial: the mapper and the reducer become methods of our `MRWordFreqCount` class that inherits from the MRJob class.\n",
    "\n",
    "The script can be run in local mode with the following command:\n",
    "\n",
    "```bash\n",
    "python scripts/mrjob_wc.py data/project_gutenberg/1184-0.txt\n",
    "```\n",
    "\n",
    "Notice that it executes immediately on our VM. This is because if run in local mode MRJob will not use Hadoop to do the map-reduce. This will be very fast on small data, but become increasingly slow if data size increases.\n",
    "\n",
    "We can switch to hadoop mode by simply using the flag `-r hadoop` like this:\n",
    "\n",
    "```bash\n",
    "python scripts/mrjob_wc.py data/project_gutenberg/1184-0.txt -r hadoop\n",
    "```\n",
    "\n",
    "As you can see this wraps around hadoop streaming and it automates all the steps we did manually in the previous lecture including:\n",
    "\n",
    "- copying data to hdfs\n",
    "- running the data through hadoop streaming\n",
    "- copying back the output\n",
    "- removing temporary folders from hdfs\n",
    "\n",
    "Nice!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "1. Use the code above to run the Word count on the whole project_gutenberg folder using MRJob\n",
    "1. Compare the execution time for the local mode and the hadoop mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: add a combiner\n",
    "\n",
    "\n",
    "A Combiner, also known as a semi-reducer, is an optional class that operates by accepting the inputs from the Map class and thereafter passing the output key-value pairs to the Reducer class.\n",
    "\n",
    "The main function of a Combiner is to summarize the map output records with the same key. The output (key-value collection) of the combiner will be sent over the network to the actual Reducer task as input.\n",
    "\n",
    "In MRJob these can be added simply by defining a method called `combiner`. Go ahead and modify the `MR` class adding a combiner. Then run it on the `project_gutenberg` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: multi step jobs\n",
    "\n",
    "So far we've always worked with one step Map-reduce jobs. These are very simple. What if we wanted to perform a calculation that involves multiple steps? For example, what if we wanted to count the words in our documents and then find the most common word? This would involve the following steps:\n",
    "\n",
    "- map our text to a mapper that output pairs of (word, 1)\n",
    "- combine the pairs using the word as key (optional)\n",
    "- reduce the pairs using the word as key\n",
    "- find the word with the maximum count\n",
    "\n",
    "The last step can be achieved by chaining a new map reduce where the map function is the identity and the reduce function is something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reducer_find_max_word(self, _, word_count_pairs):\n",
    "    yield max(word_count_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we are aggregating over a blank key in order to get all possible word count pairs and then get the one with the maximum count.\n",
    "\n",
    "Go ahead and insert that into the MR class. In order to do that you'll need to use the `steps` function which is documented [here](https://pythonhosted.org/mrjob/job.html#mrjob.job.MRJob.steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Setup and teardown of tasks\n",
    "\n",
    "MRJob allows you to write methods to run at the beginning and end of each mapper and reducer functions: the *_init() and *_final() methods:\n",
    "\n",
    "- mapper_init()\n",
    "- combiner_init()\n",
    "- reducer_init()\n",
    "- mapper_final()\n",
    "- combiner_final()\n",
    "- reducer_final()\n",
    "\n",
    "These functions are run only once at the beginning or at the end of each mapper and reducer and are useful for example when we want to have local variables available to the mapper.\n",
    "\n",
    "For example we could use `mapper_init` to load some kind of support file, like a sqlite3 database, or perhaps create a temporary fileor variable.\n",
    "\n",
    "Go ahead and rewrite the Word count using a `mapper_init` and `mapper_final` that do the following:\n",
    "\n",
    "- mapper_init should initialize a dictionary for the words\n",
    "- mapper should add the words to the dictionary as keys and increase the count each time the same word is ecountered\n",
    "- mapper_final should yield the (word, count) pairs contained in the dictionary\n",
    "- the reducer is going to be the same as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Counters\n",
    "\n",
    "When we run a longer MR task, we may want to check the status of the calculation. This is achieved in MRJob using counters.\n",
    "\n",
    "Hadoop lets you track counters that are aggregated over a step. A counter has a group, a name, and an integer value. Hadoop itself tracks a few counters automatically. mrjob prints your jobâ€™s counters to the command line when your job finishes, and they are available to the runner object if you invoke it programmatically.\n",
    "\n",
    "To increment a counter from anywhere in your job, use the `increment_counter()` method.\n",
    "\n",
    "Go ahead and add a custom counter to the mapper function of the word count so that we know how many words it has processed. In order to see the counts, you may want to redirect the output of the job to a file or to `/dev/null` if you don't care about it.\n",
    "\n",
    "    python mrjob_counters.py data/project_gutenberg > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Putting it all together\n",
    "\n",
    "You've seen how powerful MRJob can be. Let's put it all together and write a class that returns the top 15 most frequent words in our text. We can implement this in various ways, you're free to choose the one you think is more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus 2: Use NLTK to recognize a book\n",
    "\n",
    "Let's see if we can recognize a book from the most common words.\n",
    "\n",
    "- Run a local MRJob word count on a single file in the project_gutemberg\n",
    "- save the result to a pandas dataframe\n",
    "- use nltk to filter out the common english words\n",
    "- print out the most common 40 words\n",
    "- what book is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Additional Resources\n",
    "\n",
    "- [MRJob Documentation](https://pythonhosted.org/mrjob/)\n",
    "- [MRJob Examples](https://github.com/Yelp/mrjob/tree/master/mrjob/examples)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
